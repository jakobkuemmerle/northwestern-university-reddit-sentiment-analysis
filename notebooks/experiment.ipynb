{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lindaliang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/lindaliang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'accession', 'admission', 'admissioned', 'admissioning', 'admissions', 'admittance', 'entree']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def generate_similar_words(keyword):\n",
    "    # Initialize WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Find synonyms using WordNet\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(keyword):\n",
    "        for lemma in syn.lemmas():\n",
    "            # Exclude compound words or phrases\n",
    "            if \"_\" not in lemma.name() and len(lemma.name().split()) == 1:\n",
    "                synonyms.add(lemma.name())\n",
    "    \n",
    "    # Generate morphological variations\n",
    "    variations = set([\n",
    "        keyword,  # Original word\n",
    "        lemmatizer.lemmatize(keyword, pos='n'),  # Lemma (noun)\n",
    "        lemmatizer.lemmatize(keyword, pos='v'),  # Lemma (verb)\n",
    "        lemmatizer.lemmatize(keyword, pos='a'),  # Lemma (adjective)\n",
    "        f\"{keyword}s\",  # Plural\n",
    "        f\"{keyword}ed\",  # Past tense\n",
    "        f\"{keyword}ing\"  # Present participle\n",
    "    ])\n",
    "    \n",
    "    # Combine and deduplicate\n",
    "    similar_words = sorted(synonyms.union(variations))\n",
    "    return similar_words\n",
    "\n",
    "# Example usage\n",
    "keyword = \"admission\"\n",
    "similar_words = generate_similar_words(keyword)\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['access',\n",
       " 'accession',\n",
       " 'admission',\n",
       " 'admissioned',\n",
       " 'admissioning',\n",
       " 'admissions',\n",
       " 'admittance',\n",
       " 'entree']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (1.57.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (0.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Topic 1': 'Analytics', 'Topic 2': 'Healthcare', 'Topic 3': 'Sports', 'Topic 4': 'Education', 'Topic 5': 'Finance'}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key = openai.api_key)\n",
    "\n",
    "def generate_summary_for_topics(topics: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generates one-word summaries for the given topics using OpenAI API (GPT-4).\n",
    "    \n",
    "    Args:\n",
    "        topics (dict): Dictionary with topic names and their top words.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with topic names and their one-word summaries.\n",
    "    \"\"\"\n",
    "    # Create a prompt with all the topics and their words\n",
    "    prompt = \"For each of the following topics, summarize the key idea in one word:\\n\\n\"\n",
    "    for topic, words in topics.items():\n",
    "        prompt += f\"{topic}: {', '.join(words)}\\n\"\n",
    "    \n",
    "    try:\n",
    "        # Make the API call to OpenAI (using GPT-4)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=50,  # Limit response length for better formatting\n",
    "            temperature=0.5  # Adjust creativity\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        summary = response.choices[0].message.content\n",
    "        \n",
    "        # Parse the summary into a dictionary\n",
    "        summarized_topics = {}\n",
    "        topic_summaries = summary.split(\"\\n\")\n",
    "        for line in topic_summaries:\n",
    "            if ':' in line:\n",
    "                topic_name, topic_summary = line.split(\":\", 1)\n",
    "                summarized_topics[topic_name.strip()] = topic_summary.strip()\n",
    "        \n",
    "        return summarized_topics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Example usage\n",
    "topics = {\n",
    "    \"Topic 1\": [\"data\", \"analysis\", \"machine\", \"learning\", \"algorithm\", \"model\", \"predict\", \"accuracy\", \"statistics\", \"performance\"],\n",
    "    \"Topic 2\": [\"hospital\", \"patient\", \"care\", \"health\", \"medical\", \"treatment\", \"doctor\", \"nurse\", \"clinic\", \"hospitalization\"],\n",
    "    \"Topic 3\": [\"sports\", \"team\", \"soccer\", \"goal\", \"match\", \"players\", \"tactics\", \"league\", \"competition\", \"performance\"],\n",
    "    \"Topic 4\": [\"education\", \"university\", \"learning\", \"student\", \"campus\", \"teacher\", \"classroom\", \"course\", \"degree\", \"professor\"],\n",
    "    \"Topic 5\": [\"finance\", \"economy\", \"stock\", \"investment\", \"market\", \"business\", \"capital\", \"growth\", \"interest\", \"risk\"]\n",
    "}\n",
    "\n",
    "summarized_topics = generate_summary_for_topics(topics)\n",
    "print(summarized_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
