{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lindaliang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/lindaliang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'accession', 'admission', 'admissioned', 'admissioning', 'admissions', 'admittance', 'entree']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def generate_similar_words(keyword):\n",
    "    # Initialize WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Find synonyms using WordNet\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(keyword):\n",
    "        for lemma in syn.lemmas():\n",
    "            # Exclude compound words or phrases\n",
    "            if \"_\" not in lemma.name() and len(lemma.name().split()) == 1:\n",
    "                synonyms.add(lemma.name())\n",
    "    \n",
    "    # Generate morphological variations\n",
    "    variations = set([\n",
    "        keyword,  # Original word\n",
    "        lemmatizer.lemmatize(keyword, pos='n'),  # Lemma (noun)\n",
    "        lemmatizer.lemmatize(keyword, pos='v'),  # Lemma (verb)\n",
    "        lemmatizer.lemmatize(keyword, pos='a'),  # Lemma (adjective)\n",
    "        f\"{keyword}s\",  # Plural\n",
    "        f\"{keyword}ed\",  # Past tense\n",
    "        f\"{keyword}ing\"  # Present participle\n",
    "    ])\n",
    "    \n",
    "    # Combine and deduplicate\n",
    "    similar_words = sorted(synonyms.union(variations))\n",
    "    return similar_words\n",
    "\n",
    "# Example usage\n",
    "keyword = \"admission\"\n",
    "similar_words = generate_similar_words(keyword)\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['access',\n",
       " 'accession',\n",
       " 'admission',\n",
       " 'admissioned',\n",
       " 'admissioning',\n",
       " 'admissions',\n",
       " 'admittance',\n",
       " 'entree']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (1.57.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (0.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/lindaliang/Library/Caches/pypoetry/virtualenvs/northwestern-university-reddit-sentiment-a-p8YxfkNH-py3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Topic 1': 'Analytics', 'Topic 2': 'Healthcare', 'Topic 3': 'Sports', 'Topic 4': 'Education', 'Topic 5': 'Finance'}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key = openai.api_key)\n",
    "\n",
    "def generate_summary_for_topics(topics: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generates one-word summaries for the given topics using OpenAI API (GPT-4).\n",
    "    \n",
    "    Args:\n",
    "        topics (dict): Dictionary with topic names and their top words.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with topic names and their one-word summaries.\n",
    "    \"\"\"\n",
    "    # Create a prompt with all the topics and their words\n",
    "    prompt = \"For each of the following topics, summarize the key idea in one word:\\n\\n\"\n",
    "    for topic, words in topics.items():\n",
    "        prompt += f\"{topic}: {', '.join(words)}\\n\"\n",
    "    \n",
    "    try:\n",
    "        # Make the API call to OpenAI (using GPT-4)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=50,  # Limit response length for better formatting\n",
    "            temperature=0.5  # Adjust creativity\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        summary = response.choices[0].message.content\n",
    "        \n",
    "        # Parse the summary into a dictionary\n",
    "        summarized_topics = {}\n",
    "        topic_summaries = summary.split(\"\\n\")\n",
    "        for line in topic_summaries:\n",
    "            if ':' in line:\n",
    "                topic_name, topic_summary = line.split(\":\", 1)\n",
    "                summarized_topics[topic_name.strip()] = topic_summary.strip()\n",
    "        \n",
    "        return summarized_topics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Example usage\n",
    "topics = {\n",
    "    \"Topic 1\": [\"data\", \"analysis\", \"machine\", \"learning\", \"algorithm\", \"model\", \"predict\", \"accuracy\", \"statistics\", \"performance\"],\n",
    "    \"Topic 2\": [\"hospital\", \"patient\", \"care\", \"health\", \"medical\", \"treatment\", \"doctor\", \"nurse\", \"clinic\", \"hospitalization\"],\n",
    "    \"Topic 3\": [\"sports\", \"team\", \"soccer\", \"goal\", \"match\", \"players\", \"tactics\", \"league\", \"competition\", \"performance\"],\n",
    "    \"Topic 4\": [\"education\", \"university\", \"learning\", \"student\", \"campus\", \"teacher\", \"classroom\", \"course\", \"degree\", \"professor\"],\n",
    "    \"Topic 5\": [\"finance\", \"economy\", \"stock\", \"investment\", \"market\", \"business\", \"capital\", \"growth\", \"interest\", \"risk\"]\n",
    "}\n",
    "\n",
    "summarized_topics = generate_summary_for_topics(topics)\n",
    "print(summarized_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here is analysis on a specific post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def get_api_data(subreddit_name, search_keyword, limit=1000):\n",
    "    \"\"\"\n",
    "    Function to fetch Reddit posts from a given subreddit based on a search keyword.\n",
    "\n",
    "    Parameters:\n",
    "    - subreddit_name (str): The name of the subreddit to search in.\n",
    "    - search_keyword (str): The keyword to search for in the subreddit.\n",
    "    - limit (int, optional): The maximum number of posts to fetch. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing post data (Title, Score, URL, Created, Subreddit, Text).\n",
    "    \"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "\n",
    "    # Set up Reddit API client\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.environ.get(\"REDDIT_CLIENT_ID\"),\n",
    "        client_secret=os.environ.get(\"REDDIT_CLIENT_SECRET\"),\n",
    "        user_agent='your_user_agent'\n",
    "    )\n",
    "\n",
    "    # Search for posts in the specified subreddit with the given keyword\n",
    "    posts = reddit.subreddit(subreddit_name).search(search_keyword, sort='relevance', limit=limit)\n",
    "\n",
    "    # Create an empty list to store post data\n",
    "    post_data = []\n",
    "\n",
    "    # Extract relevant data from each post\n",
    "    for post in posts:\n",
    "        post_data.append({\n",
    "            'Title': post.title,\n",
    "            'Score': post.score,\n",
    "            'URL': post.url,\n",
    "            'Created': post.created_utc,\n",
    "            'Subreddit': post.subreddit.display_name,\n",
    "            'Text': post.selftext\n",
    "        })\n",
    "\n",
    "    # Convert the list of post data to a DataFrame\n",
    "    return pd.DataFrame(post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Score</th>\n",
       "      <th>URL</th>\n",
       "      <th>Created</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Confused between MSDS &amp; MLDS</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/Northwestern/comments...</td>\n",
       "      <td>1.730719e+09</td>\n",
       "      <td>Northwestern</td>\n",
       "      <td>Why do they have two different programs for a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Honest Review of Northwestern MS in Machine Le...</td>\n",
       "      <td>32</td>\n",
       "      <td>https://www.reddit.com/r/Northwestern/comments...</td>\n",
       "      <td>1.713151e+09</td>\n",
       "      <td>Northwestern</td>\n",
       "      <td>Tldr: its pretty overrated and students are n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Score  \\\n",
       "0                       Confused between MSDS & MLDS      2   \n",
       "1  Honest Review of Northwestern MS in Machine Le...     32   \n",
       "\n",
       "                                                 URL       Created  \\\n",
       "0  https://www.reddit.com/r/Northwestern/comments...  1.730719e+09   \n",
       "1  https://www.reddit.com/r/Northwestern/comments...  1.713151e+09   \n",
       "\n",
       "      Subreddit                                               Text  \n",
       "0  Northwestern  Why do they have two different programs for a ...  \n",
       "1  Northwestern   Tldr: its pretty overrated and students are n...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_api_data('Northwestern', 'mlds', limit=1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the first row:\n",
      "the program director, Diego klabjan, is very condescending and makes decisions on a whim . the program does not really do anything to help you with getting a job . be prepared to apply to hundreds of jobs and internships online .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def summarize_first_row(df):\n",
    "    # Initialize the summarizer\n",
    "    summarizer = pipeline(\"summarization\", model=\"t5-large\",tokenizer=\"t5-large\")\n",
    "\n",
    "    # Filter rows where length of Text > 1000\n",
    "    filtered_df = df[df['Text'].str.len() > 1000]\n",
    "\n",
    "    # Sort the DataFrame by Score in descending order\n",
    "    sorted_df = filtered_df.sort_values(by='Score', ascending=False)\n",
    "\n",
    "    # Get the Text from the first row\n",
    "    text_to_summarize = sorted_df.iloc[0]['Text']\n",
    "\n",
    "    # Generate the summary of the text\n",
    "    summary = summarizer(text_to_summarize, max_length=150, min_length=50, do_sample=False)\n",
    "\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you already have your DataFrame `df` loaded with the required columns\n",
    "summary = summarize_first_row(df)\n",
    "print(\"Summary of the first row:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lindaliang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/var/folders/sj/l7b5znfx0wl9s924pbhsw5200000gn/T/ipykernel_82260/2900473816.py:57: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><body><p style=\"color:gray;\"><b>Paragraph 1 (neutral):</b><br> Tldr: its pretty overrated and students are not prioritized.</p><p style=\"color:gray;\"><b>Paragraph 2 (neutral):</b><br>I write this as a recent alumni who spoke to several current and former students. I came to this degree with a very high opinion of it. The website and job placement data over the years make it look very flashy and appealing.  </p><p style=\"color:blue;\"><b>Paragraph 3 (sadness):</b><br>Unfortunately, this outward presentation is not reflective of the true quality of the program. many students feel deceived about what they paid money for.  </p><p style=\"color:orange;\"><b>Paragraph 4 (fear):</b><br>I will start from the beginning:  </p><p style=\"color:gray;\"><b>Paragraph 5 (neutral):</b><br>When I was admitted, the name of the program was still MSiA. After students decided to attend and paid their deposits, we received an email in summer saying that they changed the name of the program to MLDS. Name is fine, but Kind of annoying that they assumed we would just be ok with this and did not tell us sooner… this was red flag number 1. The current cohort tell us that the program send them some messages of what to review in the summer and recommended reviewing on Java since they have a first quarter class which taught both Java and Python together. On the first day of this class, they were told that Java would no longer be taught and that the course would focus on Python and other random tools. Again, students who paid a lot of tuition were not included in the decision making process and were told after a decision was made. It is important to tell things like this because it can affect if students want to attend the program or not. This was red flag number 2.  </p><p style=\"color:orange;\"><b>Paragraph 6 (fear):</b><br>**Curriculum:**  </p><p style=\"color:gray;\"><b>Paragraph 7 (neutral):</b><br>One of the things that appealed to me before joining MLDS was the curriculum. Some classes are indeed taught very well, but others not so much.  </p><p style=\"color:green;\"><b>Paragraph 8 (disgust):</b><br>Some instructors are considered to be a joke yet they will teach the majority of your classes in this program:</p><p style=\"color:gray;\"><b>Paragraph 9 (neutral):</b><br>* Rebecca pop (data visualization)</p><p style=\"color:gray;\"><b>Paragraph 10 (neutral):</b><br>* Diego klabjan (intro to ML, big data, reinforcement learning elective)</p><p style=\"color:gray;\"><b>Paragraph 11 (neutral):</b><br>* Ashish pujari (data mining, deep learning, cloud engineering)</p><p style=\"color:green;\"><b>Paragraph 12 (disgust):</b><br>Another annoying thing is that the primary machine learning sequence tends to teach programming in R for some reason, which is not ideal considering Python is the dominant language.  </p><p style=\"color:gray;\"><b>Paragraph 13 (neutral):</b><br>Most courses only go into a surface level of various subjects, so be prepared to do a lot of learning on your own if you are interested in something.  </p><p style=\"color:gray;\"><b>Paragraph 14 (neutral):</b><br>One weak thing about the curriculum is that it does not really teach time series analysis. Also it does not focus on data structures and algorithms, meaning you need prior knowledge if you want to compete for more advanced data science jobs.  </p><p style=\"color:green;\"><b>Paragraph 15 (disgust):</b><br>Some TAs also seem very uncaring in their jobs. One of them would consistently show up late and end very early and would never publish materials they promised too. Another always ended early and was very unhelpful whenever they were asked questions. Some TAs were also very good to be fair.  </p><p style=\"color:red;\"><b>Paragraph 16 (anger):</b><br>**Faculty and staff / treatment of students**  </p><p style=\"color:green;\"><b>Paragraph 17 (disgust):</b><br>After spending time here, what bothers me most is the lack of respect for students who paid a lot of money to be in this program. The program director, Diego klabjan, is very condescending and makes decisions on a whim. He will randomly decide to postpone class or make a class on zoom because of a schedule conflict. This is fine one or two time, but he does it too much. During zoom meeting and class, He had his camera off and eating during the class and not paying attention to students.  </p><p style=\"color:green;\"><b>Paragraph 18 (disgust):</b><br>He also says he will record any changed class but forgets to do this, leaving students unable to know what was covered. And if you join a zoom class, you cannot see what he is writing on the board and can barely hear his voice. Better to not go to class  </p><p style=\"color:gray;\"><b>Paragraph 19 (neutral):</b><br>Also his content is very random and he jumps around from random subject to random subject. He forget to cover some units and teaches them later on. He also gives pop quizzes which ask very dumb and poorly worded questions. He does not seem like he cares about teaching at all.  </p><p style=\"color:green;\"><b>Paragraph 20 (disgust):</b><br>Ashish pujari is another professor who teaches 3 classes in the program (more than any other professor). He is nice and very smart and work at Amazon but he is an adjunct without a phd. This makes it questionable why he is teaching courses like deep learning and data mining which should be taught by phds in a top program like this. His teaching quality not great.  </p><p style=\"color:gray;\"><b>Paragraph 21 (neutral):</b><br>Topics such as deep learning and data mining are complex/very important and need to be taught in smaller time session so students can absorb the material. Instead a once a week 3 hour class feel like information overload.  </p><p style=\"color:gray;\"><b>Paragraph 22 (neutral):</b><br>Some professors also works full time, so our classes are scheduled around their availability. This cause some very strange meeting times like 6-9 pm on weeknight and 2-5 pm on Fridays. Which does not feel right. We deserve someone who will treat this as their full time job, not something they do on the side.  </p><p style=\"color:red;\"><b>Paragraph 23 (anger):</b><br>Stephen, the lead program assistant, has also been described by students mean. Many say hes nice he was during admitted students day and bootcamp but how his attitude changed once students come to campus. He is cranky if you ask him in a wrong time. It can also be hard to contact him.  </p><p style=\"color:blue;\"><b>Paragraph 24 (sadness):</b><br>In the spring, we have 4 classes, and 3 of them meet once a week for 3 hours each. Imagine having three 3 hour lectures per week. And these classes happen at night from 6-9 pm or 2-5 pm on fridays which is terrible timing. Since we pay so much tuition and we have our own private space, it is sad that we still have a schedule that makes it look like we are not important.  </p><p style=\"color:orange;\"><b>Paragraph 25 (fear):</b><br>**Practicum and capstone**  </p><p style=\"color:green;\"><b>Paragraph 26 (disgust):</b><br>Many students said they wanted to join this program for the practicum. Actually many students now saying the practicum is one of the worst parts of the program. Diego is having trouble sourcing interesting/well-thought out projects and has to accepting some bad projects or recruiting students to do “research” in his lab.  </p><p style=\"color:gray;\"><b>Paragraph 27 (neutral):</b><br>Some of the projects are decent, but most are not well designed. Your company will think u free labor and put pressure on you to do their work even when you are busy with school and job searching. It can give you something to put on your resume at least, but only if the company allows it and some do not.  </p><p style=\"color:orange;\"><b>Paragraph 28 (fear):</b><br>**Work load / group work**  </p><p style=\"color:green;\"><b>Paragraph 29 (disgust):</b><br>For some reason, almost every single course in the program has a final group project. There is so much group work and so much busy work which is too much. Sometimes it can be good, but in this program there is too much busy work and not everything needs to be a group project. It can feel like a waste of time.  </p><p style=\"color:red;\"><b>Paragraph 30 (anger):</b><br>Also in some classes groups are randomly assigned, and many students complain about getting a bad teammate who does not do any work. This causes some drama.  </p><p style=\"color:orange;\"><b>Paragraph 31 (fear):</b><br>**Cohort**  </p><p style=\"color:gray;\"><b>Paragraph 32 (neutral):</b><br>Classmate in the program are generally nice. Over time, the cohort has become very clique and there isn’t as much interaction as there was in beginning. There is also a lot of drama and gossip which can be annoying to be around.  </p><p style=\"color:red;\"><b>Paragraph 33 (anger):</b><br>Because of huge workload, students often skip class and class are very empty.  </p><p style=\"color:gray;\"><b>Paragraph 34 (neutral):</b><br>Every year the cohort size increases, showing the program just want more money.  </p><p style=\"color:orange;\"><b>Paragraph 35 (fear):</b><br>Career assistance  </p><p style=\"color:gray;\"><b>Paragraph 36 (neutral):</b><br>The program does not really do anything to help you with getting a job. The “networking” they hosted was supposed to have some confirmed companies but only 3 showed up lol and the program has been struggling to get companies to recruit directly from the program. The quality of networking events is very low and will not lead to job or anything.  </p><p style=\"color:gray;\"><b>Paragraph 37 (neutral):</b><br>Engineering career fairs and campus career fairs are too not very helpful. career services is not great.  </p><p style=\"color:orange;\"><b>Paragraph 38 (fear):</b><br>Be prepared to apply to hundreds of jobs and internships online.  </p><p style=\"color:blue;\"><b>Paragraph 39 (sadness):</b><br>Many students are struggling right now to find jobs and internships especially with the current market. The statistics on the program website are likely exaggerated.  </p><p style=\"color:orange;\"><b>Paragraph 40 (fear):</b><br>**My final opinion**  </p><p style=\"color:gray;\"><b>Paragraph 41 (neutral):</b><br>This degree is good for people without an technical background or for people with a technical background who just want to say they got a degree at northwestern. It is not a bad program and gets some decent placement, but I cannot say it is very good the way I expected. It is overrated and very expensive and students are not treated as priority.  </p></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import nltk\n",
    "\n",
    "def sentiment_analysis_by_paragraph(df):\n",
    "    # Filter rows where length of Text > 1000\n",
    "    filtered_df = df[df['Text'].str.len() > 1000]\n",
    "\n",
    "    # Sort the DataFrame by Score in descending order\n",
    "    sorted_df = filtered_df.sort_values(by='Score', ascending=False)\n",
    "\n",
    "    # Get the Text from the first row\n",
    "    text = sorted_df.iloc[0]['Text']\n",
    "    \n",
    "    # Load the pre-trained emotion classification model\n",
    "    emotion_model = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "    # Define color mapping for emotions\n",
    "    emotion_colors = {\n",
    "        'anger': 'red',\n",
    "        'disgust': 'green',\n",
    "        'fear': 'orange',\n",
    "        'joy': 'yellow',\n",
    "        'love': 'pink',\n",
    "        'sadness': 'blue',\n",
    "        'surprise': 'purple',\n",
    "        'neutral': 'gray'  # Add neutral as gray for neutral emotions\n",
    "    }\n",
    "\n",
    "\n",
    "    # Chunk the text into paragraphs\n",
    "    nltk.download('punkt')\n",
    "    paragraphs = text.split('\\n')\n",
    "\n",
    "    # Remove empty paragraphs (if any)\n",
    "    paragraphs = [p for p in paragraphs if p.strip()]\n",
    "\n",
    "    # Perform emotion detection on each paragraph\n",
    "    chunk_emotions = []\n",
    "    for chunk in paragraphs:\n",
    "        emotions = emotion_model(chunk)\n",
    "        chunk_emotions.append((chunk, emotions[0]['label']))  # Save the paragraph and its emotion label\n",
    "\n",
    "    # Generate HTML with highlighted paragraphs\n",
    "    html_output = \"<html><body>\"\n",
    "\n",
    "    for idx, (para, emotion) in enumerate(chunk_emotions):\n",
    "        color = emotion_colors.get(emotion.lower(), 'gray')  # Default to gray if emotion not found\n",
    "        html_output += f'<p style=\"color:{color};\"><b>Paragraph {idx+1} ({emotion}):</b><br>{para}</p>'\n",
    "\n",
    "    html_output += \"</body></html>\"\n",
    "\n",
    "    # Save to an HTML file or display in Jupyter\n",
    "    with open(\"highlighted_paragraphs.html\", \"w\") as f:\n",
    "        f.write(html_output)\n",
    "\n",
    "    # If you're in a Jupyter notebook, you can render it directly\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(html_output))\n",
    "    \n",
    "sentiment_analysis_by_paragraph(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lindaliang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1: [{'label': 'neutral', 'score': 0.49462947249412537}]\n",
      "Paragraph 2: [{'label': 'neutral', 'score': 0.5831292271614075}]\n",
      "Paragraph 3: [{'label': 'sadness', 'score': 0.5691455006599426}]\n",
      "Paragraph 4: [{'label': 'fear', 'score': 0.8631186485290527}]\n",
      "Paragraph 5: [{'label': 'neutral', 'score': 0.78631991147995}]\n",
      "Paragraph 6: [{'label': 'fear', 'score': 0.3016975224018097}]\n",
      "Paragraph 7: [{'label': 'neutral', 'score': 0.7004575729370117}]\n",
      "Paragraph 8: [{'label': 'disgust', 'score': 0.33836647868156433}]\n",
      "Paragraph 9: [{'label': 'neutral', 'score': 0.8757570385932922}]\n",
      "Paragraph 10: [{'label': 'neutral', 'score': 0.9419630765914917}]\n",
      "Paragraph 11: [{'label': 'neutral', 'score': 0.9168189167976379}]\n",
      "Paragraph 12: [{'label': 'disgust', 'score': 0.6286032199859619}]\n",
      "Paragraph 13: [{'label': 'neutral', 'score': 0.9650899171829224}]\n",
      "Paragraph 14: [{'label': 'neutral', 'score': 0.8378678560256958}]\n",
      "Paragraph 15: [{'label': 'disgust', 'score': 0.4352704882621765}]\n",
      "Paragraph 16: [{'label': 'anger', 'score': 0.43089738488197327}]\n",
      "Paragraph 17: [{'label': 'disgust', 'score': 0.7097668051719666}]\n",
      "Paragraph 18: [{'label': 'disgust', 'score': 0.4570872485637665}]\n",
      "Paragraph 19: [{'label': 'neutral', 'score': 0.5702400803565979}]\n",
      "Paragraph 20: [{'label': 'disgust', 'score': 0.736876368522644}]\n",
      "Paragraph 21: [{'label': 'neutral', 'score': 0.7956421375274658}]\n",
      "Paragraph 22: [{'label': 'neutral', 'score': 0.6344045996665955}]\n",
      "Paragraph 23: [{'label': 'anger', 'score': 0.5592345595359802}]\n",
      "Paragraph 24: [{'label': 'sadness', 'score': 0.6096447706222534}]\n",
      "Paragraph 25: [{'label': 'fear', 'score': 0.5304836630821228}]\n",
      "Paragraph 26: [{'label': 'disgust', 'score': 0.8893448114395142}]\n",
      "Paragraph 27: [{'label': 'neutral', 'score': 0.6118414998054504}]\n",
      "Paragraph 28: [{'label': 'fear', 'score': 0.5001435875892639}]\n",
      "Paragraph 29: [{'label': 'disgust', 'score': 0.7452468276023865}]\n",
      "Paragraph 30: [{'label': 'anger', 'score': 0.4126776456832886}]\n",
      "Paragraph 31: [{'label': 'fear', 'score': 0.547913134098053}]\n",
      "Paragraph 32: [{'label': 'neutral', 'score': 0.5559526681900024}]\n",
      "Paragraph 33: [{'label': 'anger', 'score': 0.30412381887435913}]\n",
      "Paragraph 34: [{'label': 'neutral', 'score': 0.8246845006942749}]\n",
      "Paragraph 35: [{'label': 'fear', 'score': 0.8332236409187317}]\n",
      "Paragraph 36: [{'label': 'neutral', 'score': 0.5291414260864258}]\n",
      "Paragraph 37: [{'label': 'neutral', 'score': 0.3578794598579407}]\n",
      "Paragraph 38: [{'label': 'fear', 'score': 0.5010673999786377}]\n",
      "Paragraph 39: [{'label': 'sadness', 'score': 0.4319019913673401}]\n",
      "Paragraph 40: [{'label': 'fear', 'score': 0.48648637533187866}]\n",
      "Paragraph 41: [{'label': 'neutral', 'score': 0.6257749795913696}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained emotion classification model\n",
    "emotion_model = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "# Chunk the text (e.g., into sentences)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Step 1: Split text by newline characters into paragraphs\n",
    "paragraphs = text.split('\\n')\n",
    "\n",
    "# Step 2: Remove empty paragraphs (if there are any)\n",
    "paragraphs = [p for p in paragraphs if p.strip()]\n",
    "\n",
    "# Step 3: Define chunk size (number of paragraphs per chunk)\n",
    "chunk_size = 1 \n",
    "\n",
    "# Step 4: Split paragraphs into chunks\n",
    "chunks = [paragraphs[i:i + chunk_size] for i in range(0, len(paragraphs), chunk_size)]\n",
    "\n",
    "# Perform emotion detection on each chunk\n",
    "chunk_emotions = []\n",
    "for chunk in chunks:\n",
    "    chunk_text = ' '.join(chunk)\n",
    "    emotions = emotion_model(chunk_text)\n",
    "    chunk_emotions.append(emotions)\n",
    "\n",
    "# Output emotions for each chunk\n",
    "for idx, emotions in enumerate(chunk_emotions):\n",
    "    print(f\"Paragraph {idx+1}: {emotions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different paragraph chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lindaliang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/var/folders/sj/l7b5znfx0wl9s924pbhsw5200000gn/T/ipykernel_82260/264519026.py:61: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><body><p style=\"color:gray;\"><b>Paragraph 1 (neutral):</b><br> Tldr: its pretty overrated and students are not prioritized. I write this as a recent alumni who spoke to several current and former students. I came to this degree with a very high opinion of it. The website and job placement data over the years make it look very flashy and appealing.  </p><p style=\"color:blue;\"><b>Paragraph 2 (sadness):</b><br>Unfortunately, this outward presentation is not reflective of the true quality of the program. many students feel deceived about what they paid money for.   I will start from the beginning:  </p><p style=\"color:gray;\"><b>Paragraph 3 (neutral):</b><br>When I was admitted, the name of the program was still MSiA. After students decided to attend and paid their deposits, we received an email in summer saying that they changed the name of the program to MLDS. Name is fine, but Kind of annoying that they assumed we would just be ok with this and did not tell us sooner… this was red flag number 1. The current cohort tell us that the program send them some messages of what to review in the summer and recommended reviewing on Java since they have a first quarter class which taught both Java and Python together. On the first day of this class, they were told that Java would no longer be taught and that the course would focus on Python and other random tools. Again, students who paid a lot of tuition were not included in the decision making process and were told after a decision was made. It is important to tell things like this because it can affect if students want to attend the program or not. This was red flag number 2.  </p><p style=\"color:gray;\"><b>Paragraph 4 (neutral):</b><br>**Curriculum:**   One of the things that appealed to me before joining MLDS was the curriculum. Some classes are indeed taught very well, but others not so much.  </p><p style=\"color:gray;\"><b>Paragraph 5 (neutral):</b><br>Some instructors are considered to be a joke yet they will teach the majority of your classes in this program: * Rebecca pop (data visualization)</p><p style=\"color:gray;\"><b>Paragraph 6 (neutral):</b><br>* Diego klabjan (intro to ML, big data, reinforcement learning elective) * Ashish pujari (data mining, deep learning, cloud engineering)</p><p style=\"color:green;\"><b>Paragraph 7 (disgust):</b><br>Another annoying thing is that the primary machine learning sequence tends to teach programming in R for some reason, which is not ideal considering Python is the dominant language.  </p><p style=\"color:gray;\"><b>Paragraph 8 (neutral):</b><br>Most courses only go into a surface level of various subjects, so be prepared to do a lot of learning on your own if you are interested in something.  </p><p style=\"color:gray;\"><b>Paragraph 9 (neutral):</b><br>One weak thing about the curriculum is that it does not really teach time series analysis. Also it does not focus on data structures and algorithms, meaning you need prior knowledge if you want to compete for more advanced data science jobs.  </p><p style=\"color:green;\"><b>Paragraph 10 (disgust):</b><br>Some TAs also seem very uncaring in their jobs. One of them would consistently show up late and end very early and would never publish materials they promised too. Another always ended early and was very unhelpful whenever they were asked questions. Some TAs were also very good to be fair.  </p><p style=\"color:green;\"><b>Paragraph 11 (disgust):</b><br>**Faculty and staff / treatment of students**   After spending time here, what bothers me most is the lack of respect for students who paid a lot of money to be in this program. The program director, Diego klabjan, is very condescending and makes decisions on a whim. He will randomly decide to postpone class or make a class on zoom because of a schedule conflict. This is fine one or two time, but he does it too much. During zoom meeting and class, He had his camera off and eating during the class and not paying attention to students.  </p><p style=\"color:green;\"><b>Paragraph 12 (disgust):</b><br>He also says he will record any changed class but forgets to do this, leaving students unable to know what was covered. And if you join a zoom class, you cannot see what he is writing on the board and can barely hear his voice. Better to not go to class  </p><p style=\"color:gray;\"><b>Paragraph 13 (neutral):</b><br>Also his content is very random and he jumps around from random subject to random subject. He forget to cover some units and teaches them later on. He also gives pop quizzes which ask very dumb and poorly worded questions. He does not seem like he cares about teaching at all.  </p><p style=\"color:green;\"><b>Paragraph 14 (disgust):</b><br>Ashish pujari is another professor who teaches 3 classes in the program (more than any other professor). He is nice and very smart and work at Amazon but he is an adjunct without a phd. This makes it questionable why he is teaching courses like deep learning and data mining which should be taught by phds in a top program like this. His teaching quality not great.  </p><p style=\"color:gray;\"><b>Paragraph 15 (neutral):</b><br>Topics such as deep learning and data mining are complex/very important and need to be taught in smaller time session so students can absorb the material. Instead a once a week 3 hour class feel like information overload.  </p><p style=\"color:gray;\"><b>Paragraph 16 (neutral):</b><br>Some professors also works full time, so our classes are scheduled around their availability. This cause some very strange meeting times like 6-9 pm on weeknight and 2-5 pm on Fridays. Which does not feel right. We deserve someone who will treat this as their full time job, not something they do on the side.  </p><p style=\"color:red;\"><b>Paragraph 17 (anger):</b><br>Stephen, the lead program assistant, has also been described by students mean. Many say hes nice he was during admitted students day and bootcamp but how his attitude changed once students come to campus. He is cranky if you ask him in a wrong time. It can also be hard to contact him.  </p><p style=\"color:blue;\"><b>Paragraph 18 (sadness):</b><br>In the spring, we have 4 classes, and 3 of them meet once a week for 3 hours each. Imagine having three 3 hour lectures per week. And these classes happen at night from 6-9 pm or 2-5 pm on fridays which is terrible timing. Since we pay so much tuition and we have our own private space, it is sad that we still have a schedule that makes it look like we are not important.  </p><p style=\"color:green;\"><b>Paragraph 19 (disgust):</b><br>**Practicum and capstone**   Many students said they wanted to join this program for the practicum. Actually many students now saying the practicum is one of the worst parts of the program. Diego is having trouble sourcing interesting/well-thought out projects and has to accepting some bad projects or recruiting students to do “research” in his lab.  </p><p style=\"color:gray;\"><b>Paragraph 20 (neutral):</b><br>Some of the projects are decent, but most are not well designed. Your company will think u free labor and put pressure on you to do their work even when you are busy with school and job searching. It can give you something to put on your resume at least, but only if the company allows it and some do not.  </p><p style=\"color:green;\"><b>Paragraph 21 (disgust):</b><br>**Work load / group work**   For some reason, almost every single course in the program has a final group project. There is so much group work and so much busy work which is too much. Sometimes it can be good, but in this program there is too much busy work and not everything needs to be a group project. It can feel like a waste of time.  </p><p style=\"color:red;\"><b>Paragraph 22 (anger):</b><br>Also in some classes groups are randomly assigned, and many students complain about getting a bad teammate who does not do any work. This causes some drama.  </p><p style=\"color:yellow;\"><b>Paragraph 23 (joy):</b><br>**Cohort**   Classmate in the program are generally nice. Over time, the cohort has become very clique and there isn’t as much interaction as there was in beginning. There is also a lot of drama and gossip which can be annoying to be around.  </p><p style=\"color:orange;\"><b>Paragraph 24 (fear):</b><br>Because of huge workload, students often skip class and class are very empty.   Every year the cohort size increases, showing the program just want more money.  </p><p style=\"color:blue;\"><b>Paragraph 25 (sadness):</b><br>Career assistance   The program does not really do anything to help you with getting a job. The “networking” they hosted was supposed to have some confirmed companies but only 3 showed up lol and the program has been struggling to get companies to recruit directly from the program. The quality of networking events is very low and will not lead to job or anything.  </p><p style=\"color:orange;\"><b>Paragraph 26 (fear):</b><br>Engineering career fairs and campus career fairs are too not very helpful. career services is not great.   Be prepared to apply to hundreds of jobs and internships online.  </p><p style=\"color:blue;\"><b>Paragraph 27 (sadness):</b><br>Many students are struggling right now to find jobs and internships especially with the current market. The statistics on the program website are likely exaggerated.  </p><p style=\"color:gray;\"><b>Paragraph 28 (neutral):</b><br>**My final opinion**   This degree is good for people without an technical background or for people with a technical background who just want to say they got a degree at northwestern. It is not a bad program and gets some decent placement, but I cannot say it is very good the way I expected. It is overrated and very expensive and students are not treated as priority.  </p></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import nltk\n",
    "\n",
    "# Load the pre-trained emotion classification model\n",
    "emotion_model = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "# Define color mapping for emotions\n",
    "emotion_colors = {\n",
    "    'anger': 'red',\n",
    "    'disgust': 'green',\n",
    "    'fear': 'orange',\n",
    "    'joy': 'yellow',\n",
    "    'love': 'pink',\n",
    "    'sadness': 'blue',\n",
    "    'surprise': 'purple',\n",
    "    'neutral': 'gray'\n",
    "}\n",
    "\n",
    "# Chunk the text into paragraphs\n",
    "nltk.download('punkt')\n",
    "# Split the text into paragraphs\n",
    "paragraphs = text.split('\\n')\n",
    "\n",
    "# Remove empty paragraphs (if any)\n",
    "paragraphs = [p for p in paragraphs if p.strip()]\n",
    "\n",
    "# Merge paragraphs with fewer than 20 words with the next paragraph\n",
    "merged_paragraphs = []\n",
    "i = 0\n",
    "while i < len(paragraphs):\n",
    "    # Check if the paragraph has fewer than 25 words and is not the last one\n",
    "    if len(paragraphs[i].split()) < 25 and i < len(paragraphs) - 1:\n",
    "        # Merge with the next paragraph\n",
    "        merged_paragraphs.append(paragraphs[i] + ' ' + paragraphs[i + 1])\n",
    "        i += 2  # Skip the next paragraph as it's merged\n",
    "    else:\n",
    "        # Keep the paragraph as is\n",
    "        merged_paragraphs.append(paragraphs[i])\n",
    "        i += 1\n",
    "\n",
    "# Perform emotion detection on each merged paragraph\n",
    "chunk_emotions = []\n",
    "for chunk in merged_paragraphs:\n",
    "    emotions = emotion_model(chunk)\n",
    "    chunk_emotions.append((chunk, emotions[0]['label']))  # Save the paragraph and its emotion label\n",
    "\n",
    "# Generate HTML with highlighted paragraphs\n",
    "html_output = \"<html><body>\"\n",
    "\n",
    "for idx, (para, emotion) in enumerate(chunk_emotions):\n",
    "    color = emotion_colors.get(emotion.lower(), 'gray')  # Default to gray if emotion not found\n",
    "    html_output += f'<p style=\"color:{color};\"><b>Paragraph {idx+1} ({emotion}):</b><br>{para}</p>'\n",
    "\n",
    "html_output += \"</body></html>\"\n",
    "\n",
    "# Save to an HTML file or display in Jupyter\n",
    "with open(\"highlighted_paragraphs.html\", \"w\") as f:\n",
    "    f.write(html_output)\n",
    "\n",
    "# If you're in a Jupyter notebook, you can render it directly\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement deepmoji (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for deepmoji\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deepmoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepmoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepmoji\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepMoji\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepmoji\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the DeepMoji model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepmoji'"
     ]
    }
   ],
   "source": [
    "from deepmoji import DeepMoji\n",
    "from deepmoji.tokenizer import tokenizer\n",
    "\n",
    "# Initialize the DeepMoji model\n",
    "model = DeepMoji.load_pretrained()\n",
    "\n",
    "# Tokenize and classify a chunk of text\n",
    "tokens = tokenizer.encode(text)\n",
    "emotion_scores = model.predict([tokens])\n",
    "\n",
    "# Output the detailed emotion scores\n",
    "print(emotion_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
